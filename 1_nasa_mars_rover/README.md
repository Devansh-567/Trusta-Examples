# ðŸ›°ï¸ NASA Mars Rover Anomaly Detection with `trustra`

> **"Can we trust the AI that decides if the Mars Rover should drill?"**

This project simulates a **mission-critical AI system** for NASAâ€™s Mars Rover, using [`trustra`](https://pypi.org/project/trustra/) to ensure **trust, fairness, and reliability** in autonomous decisions.

It demonstrates how `trustra` automatically:
- ðŸ” Detects data leakage
- âš–ï¸ Audits fairness (e.g., sensor bias)
- ðŸ“‰ Flags data drift
- ðŸ“Š Generates a full **auditable HTML trust report**
- ðŸš€ Delivers high performance with one `fit()` call

## ðŸš€ Quick Start

Run this to launch the Mars mission simulation:

```bash
# Clone the project
git clone https://github.com/Devansh-567/Trusta-Examples.git
cd Trusta-Examples/1_nasa_mars_rover

# Install trustra (published on PyPI)
pip install trustra

# Run the Mars Rover AI
python nasa_mars_rover_analysis.py
```
---
## ðŸ“Š Sample Output
```python
============================================================
âœ… TRUSTRA MISSION REPORT COMPLETE
ðŸ“ˆ CV AUC: 0.995
âš–ï¸  Fairness Issues: 0
ðŸ” Total Issues Found: 0
ðŸ”§ Best Model: LR
ðŸ“„ Full Report: trustra_report.html
============================================================
```
## ðŸ§ª Simulated Risks (Automatically Detected)
> âš–ï¸ Sensor ID bias (rover_sensor_id)
> ðŸ”¥ Data leakage in spectral_450nm
> ðŸ“‰ Drift between training and validation
---

## ðŸŒ Why This Matters
This isn't just a demo â€” it's a blueprint for trustworthy AI in space, healthcare, finance, and beyond.
> trustra ensures that when the AI says "drill here", scientists can trust the decision â€” no black boxes, no surprises.
